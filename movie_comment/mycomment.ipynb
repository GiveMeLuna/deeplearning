{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "[(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "[reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "x_train=vectorize_sequences(train_data)     #将训练数据向量化\n",
    "#比如x_train[0]中的内容是[0,1,1...,0,0]，含义是train_data[0]中某个单词的索引数字为i，x_train[0][i]便等于1\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment=\"5 points, the original script is really excellent, so that every modification and localization of the domestic version, let a person feel superfluous and destroy the rhythm. Especially a few key turns and breaking points, the play is not completely out, a bit of a waste of such a good set. The original version is actually a set of chamber thriller, and every change makes people jumpy. This time, the dramatic treatment and all kinds of self-righteous chicken soup and warmth have destroyed the original plot depth. And is it really the cell phone that is to blame? It's also strange. Finally, as an actress, mengyao xi has a long, long, long way to go.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 457, 2, 4, 204, 229, 9, 66, 2, 38, 15, 175, 5, 2, 7, 4, 4816, 2, 387, 6, 415, 235, 8697, 5, 2330, 4, 2, 2, 6, 171, 1317, 505, 5, 2244, 2, 4, 297, 9, 24, 340, 2, 6, 227, 7, 6, 437, 7, 141, 6, 52, 2, 2, 204, 310, 9, 165, 6, 270, 7, 7201, 2, 5, 175, 653, 166, 84, 2, 2, 2, 4, 905, 2199, 5, 32, 2572, 7, 2, 5145, 5772, 5, 4830, 28, 2730, 4, 204, 114, 2, 2, 9, 12, 66, 4, 2768, 1699, 15, 9, 8, 2, 2, 82, 2, 2, 17, 35, 2, 2, 47, 6, 2, 2, 196, 96, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "testcomment=test_comment.split()\n",
    "mycomment=[]\n",
    "mycomment.append(1)\n",
    "for i in testcomment:\n",
    "    if word_index.get(i):\n",
    "        if(word_index[i]+3<10000):\n",
    "            mycomment.append(word_index[i]+3)\n",
    "#        print(word_index[i]+3)\n",
    "    else: \n",
    "#        print('2')\n",
    "        mycomment.append(2)\n",
    "print(mycomment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_comment = vectorize_sequences([mycomment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 2s 97us/step - loss: 0.4496 - acc: 0.8165\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 2s 82us/step - loss: 0.2552 - acc: 0.9095\n",
      "25000/25000 [==============================] - 2s 78us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='binary_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22343622]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(my_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}